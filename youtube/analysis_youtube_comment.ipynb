{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji wordcloud"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XNHY00lA4az",
        "outputId": "30c8cdbf-2d41-4ecb-ac66-c910b13f813a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.14.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from wordcloud) (1.26.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from wordcloud) (11.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from wordcloud) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import re\n",
        "import emoji"
      ],
      "metadata": {
        "id": "GIyzhAKxA4z4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize sentiment analysis model\n",
        "print(\"Loading sentiment analysis model...\")\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Read CSV files\n",
        "print(\"\\nReading CSV files...\")\n",
        "try:\n",
        "    # Read the first CSV file\n",
        "    file1_path = '/content/dataset_youtube-comments-scraper_2025-06-17_06-57-22-932.csv'\n",
        "    df1 = pd.read_csv(file1_path)\n",
        "    print(f\"Successfully read {len(df1)} records from the first file\")\n",
        "\n",
        "    # Read the second CSV file\n",
        "    file2_path = '/content/dataset_youtube-comments-scraper_2025-06-17_07-02-35-990.csv'  # Replace with your second file path\n",
        "    df2 = pd.read_csv(file2_path)\n",
        "    print(f\"Successfully read {len(df2)} records from the second file\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: File not found - {e.filename}\")\n",
        "    print(\"Please ensure the CSV file exists in the specified directory\")\n",
        "    exit()\n",
        "\n",
        "# Merge the two DataFrames\n",
        "print(\"\\nMerging the two datasets...\")\n",
        "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
        "print(f\"Successfully merged {len(merged_df)} records\")\n",
        "\n",
        "# Check dataset overview\n",
        "print(f\"\\nThe dataset contains {len(merged_df.columns)} columns\")\n",
        "print(f\"Does 'comment' column exist: {'comment' in merged_df.columns}\")\n",
        "\n",
        "# Extract comment column and filter out null values\n",
        "comments = merged_df['comment'].dropna().tolist()\n",
        "print(f\"\\nFound {len(comments)} valid comments\")\n",
        "\n",
        "# Display first 3 example comments\n",
        "print(\"\\nExample Comments:\")\n",
        "for i, comment in enumerate(comments[:3]):\n",
        "    print(f\"{i+1}. {comment[:100]}...\")\n",
        "\n",
        "# Preprocess comments: replace emojis, remove URLs, etc.\n",
        "def preprocess_comment(comment):\n",
        "    # Replace emojis with text descriptions\n",
        "    comment = emoji.demojize(comment)\n",
        "    comment = comment.replace(\":\", \"\").replace(\"_\", \" \")\n",
        "\n",
        "    # Remove URLs\n",
        "    comment = re.sub(r'http\\S+', '', comment)\n",
        "\n",
        "    # Remove @usernames\n",
        "    comment = re.sub(r'@\\w+', '', comment)\n",
        "\n",
        "    # Remove special characters\n",
        "    comment = re.sub(r'[^\\w\\s]', '', comment)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    comment = comment.lower()\n",
        "\n",
        "    # Remove extra spaces\n",
        "    comment = re.sub(r'\\s+', ' ', comment).strip()\n",
        "\n",
        "    return comment\n",
        "\n",
        "print(\"\\nPreprocessing comments...\")\n",
        "processed_comments = [preprocess_comment(comment) for comment in tqdm(comments)]\n",
        "\n",
        "# Analyze sentiment\n",
        "print(\"\\nStarting sentiment analysis (this may take a few minutes)...\")\n",
        "results = []\n",
        "\n",
        "# Use progress bar\n",
        "for comment, proc_comment in tqdm(zip(comments, processed_comments), desc=\"Analysis Progress\", total=len(comments)):\n",
        "    try:\n",
        "        # Truncate overly long text (model has length limit)\n",
        "        truncated_text = proc_comment[:512] if len(proc_comment) > 512 else proc_comment\n",
        "\n",
        "        # Analyze sentiment\n",
        "        result = classifier(truncated_text)[0]\n",
        "        results.append({\n",
        "            'original_text': comment,\n",
        "            'processed_text': proc_comment,\n",
        "            'sentiment': result['label'],\n",
        "            'confidence': result['score']\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle possible errors\n",
        "        print(f\"Analysis error: {str(e)}\")\n",
        "        results.append({\n",
        "            'original_text': comment,\n",
        "            'processed_text': proc_comment,\n",
        "            'sentiment': 'ERROR',\n",
        "            'confidence': 0.0\n",
        "        })\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Statistical analysis\n",
        "print(\"\\n=== Sentiment Analysis Statistics ===\")\n",
        "sentiment_counts = results_df['sentiment'].value_counts()\n",
        "print(sentiment_counts)\n",
        "print(f\"\\nPositive Proportion: {(sentiment_counts.get('POSITIVE', 0) / len(results_df) * 100):.1f}%\")\n",
        "print(f\"Negative Proportion: {(sentiment_counts.get('NEGATIVE', 0) / len(results_df) * 100):.1f}%\")\n",
        "\n",
        "# Visualize sentiment distribution\n",
        "def visualize_sentiment_distribution(sentiment_df):\n",
        "    \"\"\"Create pie chart and bar chart for sentiment analysis results\"\"\"\n",
        "    # Sentiment count\n",
        "    counts = sentiment_df['sentiment'].value_counts()\n",
        "\n",
        "    # Create figure with two subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    # Pie chart: sentiment distribution\n",
        "    ax1.pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=90,\n",
        "            colors=['lightgreen', 'salmon', 'lightgray'])\n",
        "    ax1.axis('equal')  # Equal aspect ratio ensures pie is circular\n",
        "    ax1.set_title('Sentiment Distribution')\n",
        "\n",
        "    # Bar chart: average confidence by sentiment\n",
        "    confidence_by_sentiment = sentiment_df.groupby('sentiment')['confidence'].mean()\n",
        "    ax2.bar(confidence_by_sentiment.index, confidence_by_sentiment.values,\n",
        "            color=['lightgreen', 'salmon', 'lightgray'])\n",
        "    ax2.set_ylim(0.8, 1.0)  # Set y-axis range to better show confidence differences\n",
        "    ax2.set_title('Average Confidence by Sentiment')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('sentiment_visualization.png')\n",
        "    plt.close()\n",
        "    print(\"\\nSentiment distribution visualization saved as 'sentiment_visualization.png'\")\n",
        "\n",
        "# Analyze keywords for positive and negative comments\n",
        "def analyze_keywords(sentiment_df, top_n=10):\n",
        "    \"\"\"Analyze top keywords for positive and negative sentiments\"\"\"\n",
        "    # English stopwords\n",
        "    stopwords = set(['the', 'and', 'to', 'of', 'a', 'in', 'that', 'it', 'with', 'as', 'for', 'on', 'at', 'by', 'be', 'this', 'is', 'are', 'was', 'were', 'i', 'you', 'he', 'she', 'we', 'they', 'my', 'your', 'his', 'her', 'our', 'their'])\n",
        "\n",
        "    # Positive comments\n",
        "    positive_comments = sentiment_df[sentiment_df['sentiment'] == 'POSITIVE']['processed_text'].tolist()\n",
        "    positive_words = []\n",
        "    for comment in positive_comments:\n",
        "        words = comment.split()\n",
        "        positive_words.extend([word for word in words if word not in stopwords and len(word) > 2])\n",
        "    positive_word_counts = Counter(positive_words)\n",
        "    top_positive = positive_word_counts.most_common(top_n)\n",
        "\n",
        "    # Negative comments\n",
        "    negative_comments = sentiment_df[sentiment_df['sentiment'] == 'NEGATIVE']['processed_text'].tolist()\n",
        "    negative_words = []\n",
        "    for comment in negative_comments:\n",
        "        words = comment.split()\n",
        "        negative_words.extend([word for word in words if word not in stopwords and len(word) > 2])\n",
        "    negative_word_counts = Counter(negative_words)\n",
        "    top_negative = negative_word_counts.most_common(top_n)\n",
        "\n",
        "    return top_positive, top_negative\n",
        "\n",
        "# Generate word clouds\n",
        "def generate_wordcloud(keywords, title, filename):\n",
        "    \"\"\"Generate word cloud from keyword list\"\"\"\n",
        "    word_dict = dict(keywords)\n",
        "    wordcloud = WordCloud(\n",
        "        width=800, height=400, background_color='white', max_words=100\n",
        "    ).generate_from_frequencies(word_dict)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(title, fontsize=15)\n",
        "    plt.tight_layout(pad=0)\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "    print(f\"Word cloud saved as '{filename}'\")\n",
        "\n",
        "# View high-confidence positive and negative samples\n",
        "print(\"\\n=== Top 5 High-Confidence Positive Samples ===\")\n",
        "positive_samples = results_df[results_df['sentiment'] == 'POSITIVE'].nlargest(5, 'confidence')\n",
        "for idx, row in positive_samples.iterrows():\n",
        "    print(f\"Confidence: {row['confidence']:.2%}\")\n",
        "    print(f\"Text: {row['original_text'][:150]}...\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"\\n=== Top 5 High-Confidence Negative Samples ===\")\n",
        "negative_samples = results_df[results_df['sentiment'] == 'NEGATIVE'].nlargest(5, 'confidence')\n",
        "for idx, row in negative_samples.iterrows():\n",
        "    print(f\"Confidence: {row['confidence']:.2%}\")\n",
        "    print(f\"Text: {row['original_text'][:150]}...\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Save results\n",
        "output_file = 'merged_youtube_sentiment_results.csv'\n",
        "results_df.to_csv(output_file, index=False, encoding='utf-8')\n",
        "print(f\"\\nResults saved to: {output_file}\")\n",
        "\n",
        "# Optional: Merge sentiment results back into the original data\n",
        "df_with_sentiment = merged_df.copy()\n",
        "df_with_sentiment['sentiment'] = None\n",
        "df_with_sentiment['sentiment_confidence'] = None\n",
        "\n",
        "# Match and update sentiment results\n",
        "for idx, row in df_with_sentiment.iterrows():\n",
        "    if pd.notna(row['comment']):\n",
        "        matching_result = results_df[results_df['original_text'] == row['comment']]\n",
        "        if not matching_result.empty:\n",
        "            df_with_sentiment.at[idx, 'sentiment'] = matching_result.iloc[0]['sentiment']\n",
        "            df_with_sentiment.at[idx, 'sentiment_confidence'] = matching_result.iloc[0]['confidence']\n",
        "\n",
        "# Save complete data\n",
        "complete_output_file = 'merged_youtube_complete_with_sentiment.csv'\n",
        "df_with_sentiment.to_csv(complete_output_file, index=False, encoding='utf-8')\n",
        "print(f\"Complete data with sentiment saved to: {complete_output_file}\")\n",
        "\n",
        "# Create visualizations\n",
        "try:\n",
        "    print(\"\\nCreating visualizations...\")\n",
        "    visualize_sentiment_distribution(results_df)\n",
        "\n",
        "    # Analyze keywords\n",
        "    top_positive, top_negative = analyze_keywords(results_df)\n",
        "\n",
        "    # Generate word clouds (requires wordcloud library)\n",
        "    try:\n",
        "        generate_wordcloud(top_positive, 'Positive Sentiment Keywords', 'positive_wordcloud.png')\n",
        "        generate_wordcloud(top_negative, 'Negative Sentiment Keywords', 'negative_wordcloud.png')\n",
        "    except ImportError:\n",
        "        print(\"WordCloud library not installed. Skipping word cloud generation.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Visualization error: {str(e)}\")\n",
        "\n",
        "print(\"\\nAnalysis Complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ionF7ea4B6NN",
        "outputId": "4ab7ea14-f199-4c82-a0c5-f88f7efe2aab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading sentiment analysis model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Reading CSV files...\n",
            "Successfully read 993 records from the first file\n",
            "Successfully read 1459 records from the second file\n",
            "\n",
            "Merging the two datasets...\n",
            "Successfully merged 2452 records\n",
            "\n",
            "The dataset contains 14 columns\n",
            "Does 'comment' column exist: True\n",
            "\n",
            "Found 2452 valid comments\n",
            "\n",
            "Example Comments:\n",
            "1. infold pls restuin q dgn ayel...\n",
            "2. I keep coming back to this üò≠...\n",
            "3. \"Put this on Spotify pleaseeeee\" \n",
            "Me who uses Youtube Music cannot relate. :')...\n",
            "\n",
            "Preprocessing comments...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2452/2452 [00:00<00:00, 3448.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting sentiment analysis (this may take a few minutes)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analysis Progress:   0%|          | 10/2452 [00:00<02:37, 15.46it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Analysis Progress: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2452/2452 [00:28<00:00, 86.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Sentiment Analysis Statistics ===\n",
            "sentiment\n",
            "POSITIVE    1300\n",
            "NEGATIVE    1152\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Positive Proportion: 53.0%\n",
            "Negative Proportion: 47.0%\n",
            "\n",
            "=== Top 5 High-Confidence Positive Samples ===\n",
            "Confidence: 99.99%\n",
            "Text: This song made me have a good mood today üíñ...\n",
            "--------------------------------------------------\n",
            "Confidence: 99.99%\n",
            "Text: Beautiful... ‚ù§ Happy spring time to everyone...\n",
            "--------------------------------------------------\n",
            "Confidence: 99.99%\n",
            "Text: Yesss\n",
            "I love this vibe! üíñ...\n",
            "--------------------------------------------------\n",
            "Confidence: 99.99%\n",
            "Text: This song is just so dreamy and fresh. Perfect for spring....\n",
            "--------------------------------------------------\n",
            "Confidence: 99.99%\n",
            "Text: BEAUTIFULL! GLORIOUS! THIS IS JUST WHAT I NEEDED!!!...\n",
            "--------------------------------------------------\n",
            "\n",
            "=== Top 5 High-Confidence Negative Samples ===\n",
            "Confidence: 99.98%\n",
            "Text: ¬†@Saskia666¬† Over time, this is no longer a mobile game, but more like a PC game forced to be played on a mobile phone. Poor my phone.......\n",
            "--------------------------------------------------\n",
            "Confidence: 99.98%\n",
            "Text: This really is one of their worst songs, I hope they make a good ost again soon üò≠...\n",
            "--------------------------------------------------\n",
            "Confidence: 99.98%\n",
            "Text: AS A ZAYNE/SYLUS GIRLIE I'M NOT OKAY...\n",
            "--------------------------------------------------\n",
            "Confidence: 99.97%\n",
            "Text: IM SO FUCKING SORRY WHAT?????????...\n",
            "--------------------------------------------------\n",
            "Confidence: 99.97%\n",
            "Text: I'm still mad! I've been trying to get Lumiere second card for the whole week! I was at 9 pity but time was already over! It finished too soon and I w...\n",
            "--------------------------------------------------\n",
            "\n",
            "Results saved to: merged_youtube_sentiment_results.csv\n",
            "Complete data with sentiment saved to: merged_youtube_complete_with_sentiment.csv\n",
            "\n",
            "Creating visualizations...\n",
            "\n",
            "Sentiment distribution visualization saved as 'sentiment_visualization.png'\n",
            "Visualization error: name 'WordCloud' is not defined\n",
            "\n",
            "Analysis Complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# New: Word frequency statistics function\n",
        "def analyze_word_frequency(sentiment_df, output_file='word_frequency.csv', top_n=100):\n",
        "    \"\"\"\n",
        "    Analyze word frequency in all comments and save to CSV file\n",
        "\n",
        "    Parameters:\n",
        "        sentiment_df: DataFrame containing sentiment analysis results\n",
        "        output_file: Path to save word frequency statistics\n",
        "        top_n: Number of top frequent words to display\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Starting word frequency analysis ===\")\n",
        "\n",
        "    # Load NLTK stopwords (use built-in list if NLTK not installed)\n",
        "    try:\n",
        "        import nltk\n",
        "        from nltk.corpus import stopwords\n",
        "        nltk.download('stopwords')\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        print(\"Successfully loaded NLTK English stopwords\")\n",
        "    except:\n",
        "        # Use alternative stopword list\n",
        "        stop_words = set(['the', 'and', 'to', 'of', 'a', 'in', 'that', 'it', 'with', 'as', 'for', 'on', 'at', 'by',\n",
        "                         'be', 'this', 'is', 'are', 'was', 'were', 'i', 'you', 'he', 'she', 'we', 'they', 'my', 'your',\n",
        "                         'his', 'her', 'our', 'their', 'what', 'which', 'who', 'whom', 'these', 'those', 'am', 'an'])\n",
        "        print(\"NLTK not installed, using built-in English stopword list\")\n",
        "\n",
        "    # Extend stopword list with common meaningless words\n",
        "    stop_words.update(['video', 'like', 'just', 'get', 'make', 'one', 'people', 'go', 'would',\n",
        "                      'really', 'time', 'see', 'think', 'even', 'well', 'first', 'also', 'much'])\n",
        "\n",
        "    # Count word frequency in all comments\n",
        "    all_words = []\n",
        "    for text in sentiment_df['processed_text']:\n",
        "        if isinstance(text, str):\n",
        "            words = text.lower().split()\n",
        "            all_words.extend([word for word in words if word not in stop_words and len(word) > 2])\n",
        "\n",
        "    # Calculate word frequencies\n",
        "    word_counts = Counter(all_words)\n",
        "    total_words = len(all_words)\n",
        "\n",
        "    # Create word frequency DataFrame\n",
        "    word_freq_df = pd.DataFrame({\n",
        "        'Word': list(word_counts.keys()),\n",
        "        'Frequency': list(word_counts.values())\n",
        "    })\n",
        "\n",
        "    # Calculate frequency percentage\n",
        "    word_freq_df['Percentage'] = (word_freq_df['Frequency'] / total_words) * 100\n",
        "\n",
        "    # Sort by frequency\n",
        "    word_freq_df = word_freq_df.sort_values('Frequency', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # Save to CSV\n",
        "    word_freq_df.to_csv(output_file, index=False, encoding='utf-8')\n",
        "    print(f\"Word frequency statistics saved to: {output_file}\")\n",
        "\n",
        "    # Print top_n frequent words\n",
        "    print(f\"\\nTop {top_n} most frequent words:\")\n",
        "    print(word_freq_df.head(top_n))\n",
        "\n",
        "    return word_freq_df\n",
        "\n",
        "# New: Sentiment-based word frequency statistics\n",
        "def analyze_sentiment_word_frequency(sentiment_df, output_prefix='sentiment_word_freq', top_n=50):\n",
        "    \"\"\"\n",
        "    Analyze word frequency by sentiment and save to CSV files\n",
        "\n",
        "    Parameters:\n",
        "        sentiment_df: DataFrame containing sentiment analysis results\n",
        "        output_prefix: Prefix for output file names\n",
        "        top_n: Number of top frequent words to display per sentiment\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Starting sentiment-based word frequency analysis ===\")\n",
        "\n",
        "    # Load NLTK stopwords (use built-in list if NLTK not installed)\n",
        "    try:\n",
        "        import nltk\n",
        "        from nltk.corpus import stopwords\n",
        "        nltk.download('stopwords')\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        print(\"Successfully loaded NLTK English stopwords\")\n",
        "    except:\n",
        "        # Use alternative stopword list\n",
        "        stop_words = set(['the', 'and', 'to', 'of', 'a', 'in', 'that', 'it', 'with', 'as', 'for', 'on', 'at', 'by',\n",
        "                         'be', 'this', 'is', 'are', 'was', 'were', 'i', 'you', 'he', 'she', 'we', 'they', 'my', 'your',\n",
        "                         'his', 'her', 'our', 'their', 'what', 'which', 'who', 'whom', 'these', 'those', 'am', 'an'])\n",
        "        print(\"NLTK not installed, using built-in English stopword list\")\n",
        "\n",
        "    # Extend stopword list\n",
        "    stop_words.update(['video', 'like', 'just', 'get', 'make', 'one', 'people', 'go', 'would',\n",
        "                      'really', 'time', 'see', 'think', 'even', 'well', 'first', 'also', 'much'])\n",
        "\n",
        "    # Split by sentiment\n",
        "    positive_df = sentiment_df[sentiment_df['sentiment'] == 'POSITIVE']\n",
        "    negative_df = sentiment_df[sentiment_df['sentiment'] == 'NEGATIVE']\n",
        "\n",
        "    # Count word frequency in positive comments\n",
        "    positive_words = []\n",
        "    for text in positive_df['processed_text']:\n",
        "        if isinstance(text, str):\n",
        "            words = text.lower().split()\n",
        "            positive_words.extend([word for word in words if word not in stop_words and len(word) > 2])\n",
        "\n",
        "    # Count word frequency in negative comments\n",
        "    negative_words = []\n",
        "    for text in negative_df['processed_text']:\n",
        "        if isinstance(text, str):\n",
        "            words = text.lower().split()\n",
        "            negative_words.extend([word for word in words if word not in stop_words and len(word) > 2])\n",
        "\n",
        "    # Calculate word frequencies\n",
        "    positive_counts = Counter(positive_words)\n",
        "    negative_counts = Counter(negative_words)\n",
        "\n",
        "    # Create word frequency DataFrames\n",
        "    positive_freq_df = pd.DataFrame({\n",
        "        'Word': list(positive_counts.keys()),\n",
        "        'Frequency': list(positive_counts.values())\n",
        "    })\n",
        "\n",
        "    negative_freq_df = pd.DataFrame({\n",
        "        'Word': list(negative_counts.keys()),\n",
        "        'Frequency': list(negative_counts.values())\n",
        "    })\n",
        "\n",
        "    # Calculate frequency percentage\n",
        "    total_positive = len(positive_words)\n",
        "    total_negative = len(negative_words)\n",
        "\n",
        "    positive_freq_df['Percentage'] = (positive_freq_df['Frequency'] / total_positive) * 100\n",
        "    negative_freq_df['Percentage'] = (negative_freq_df['Frequency'] / total_negative) * 100\n",
        "\n",
        "    # Sort by frequency\n",
        "    positive_freq_df = positive_freq_df.sort_values('Frequency', ascending=False).reset_index(drop=True)\n",
        "    negative_freq_df = negative_freq_df.sort_values('Frequency', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # Save to CSV\n",
        "    positive_freq_df.to_csv(f\"{output_prefix}_positive.csv\", index=False, encoding='utf-8')\n",
        "    negative_freq_df.to_csv(f\"{output_prefix}_negative.csv\", index=False, encoding='utf-8')\n",
        "\n",
        "    print(f\"Positive sentiment word frequency saved to: {output_prefix}_positive.csv\")\n",
        "    print(f\"Negative sentiment word frequency saved to: {output_prefix}_negative.csv\")\n",
        "\n",
        "    # Print top frequent words\n",
        "    print(f\"\\nTop {top_n} most frequent words in positive comments:\")\n",
        "    print(positive_freq_df.head(top_n))\n",
        "\n",
        "    print(f\"\\nTop {top_n} most frequent words in negative comments:\")\n",
        "    print(negative_freq_df.head(top_n))\n",
        "\n",
        "    return positive_freq_df, negative_freq_df\n",
        "\n",
        "# New: Word frequency visualization\n",
        "def visualize_word_frequency(word_freq_df, title=\"Word Frequency Distribution\", output_file=\"word_frequency.png\", top_n=30):\n",
        "    \"\"\"\n",
        "    Visualize word frequency distribution\n",
        "\n",
        "    Parameters:\n",
        "        word_freq_df: Word frequency DataFrame\n",
        "        title: Chart title\n",
        "        output_file: Path to save the visualization\n",
        "        top_n: Number of top frequent words to display\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "\n",
        "        # Set font for Chinese characters (kept for compatibility, but will use available fonts)\n",
        "        # plt.rcParams[\"font.family\"] = [\"SimHei\", \"WenQuanYi Micro Hei\", \"Heiti TC\", \"Arial\"]\n",
        "\n",
        "        # Select top_n most frequent words\n",
        "        top_words = word_freq_df.head(top_n)\n",
        "\n",
        "        # Create chart\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.barplot(x=\"Frequency\", y=\"Word\", data=top_words, palette=\"viridis\")\n",
        "        plt.title(title, fontsize=15)\n",
        "        plt.xlabel(\"Frequency\", fontsize=12)\n",
        "        plt.ylabel(\"Word\", fontsize=12)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the chart\n",
        "        plt.savefig(output_file, dpi=300, bbox_inches=\"tight\")\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"Word frequency visualization saved to: {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Word frequency visualization error: {str(e)}\")\n",
        "\n",
        "# Add word frequency analysis calls at the end of the original code\n",
        "if __name__ == \"__main__\":\n",
        "    # Perform word frequency statistics\n",
        "    word_freq_df = analyze_word_frequency(results_df)\n",
        "\n",
        "    # Sentiment-based word frequency statistics\n",
        "    positive_freq_df, negative_freq_df = analyze_sentiment_word_frequency(results_df)\n",
        "\n",
        "    # Word frequency visualization\n",
        "    visualize_word_frequency(word_freq_df, \"Overall Word Frequency Distribution\", \"word_frequency.png\")\n",
        "    visualize_word_frequency(positive_freq_df, \"Positive Sentiment Word Frequency\", \"word_frequency_positive.png\")\n",
        "    visualize_word_frequency(negative_freq_df, \"Negative Sentiment Word Frequency\", \"word_frequency_negative.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7ywAN6Xd8M6",
        "outputId": "c37456fd-41e5-4449-bebb-97c89b3e3a12"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Starting word frequency analysis ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/tmp/ipython-input-4-965627367.py:177: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.barplot(x=\"Frequency\", y=\"Word\", data=top_words, palette=\"viridis\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded NLTK English stopwords\n",
            "Word frequency statistics saved to: word_frequency.csv\n",
            "\n",
            "Top 100 most frequent words:\n",
            "        Word  Frequency  Percentage\n",
            "0       face        621    3.346446\n",
            "1     crying        502    2.705179\n",
            "2   heartred        432    2.327962\n",
            "3      heart        382    2.058522\n",
            "4      zayne        272    1.465754\n",
            "..       ...        ...         ...\n",
            "95     caleb         26    0.140109\n",
            "96    popper         26    0.140109\n",
            "97      eyes         26    0.140109\n",
            "98     swift         26    0.140109\n",
            "99  thinking         26    0.140109\n",
            "\n",
            "[100 rows x 3 columns]\n",
            "\n",
            "=== Starting sentiment-based word frequency analysis ===\n",
            "Successfully loaded NLTK English stopwords\n",
            "Positive sentiment word frequency saved to: sentiment_word_freq_positive.csv\n",
            "Negative sentiment word frequency saved to: sentiment_word_freq_negative.csv\n",
            "\n",
            "Top 50 most frequent words in positive comments:\n",
            "                Word  Frequency  Percentage\n",
            "0           heartred        396    4.075332\n",
            "1               face        336    3.457857\n",
            "2              heart        315    3.241741\n",
            "3               song        206    2.119996\n",
            "4               love        204    2.099413\n",
            "5                red        140    1.440774\n",
            "6              zayne        128    1.317279\n",
            "7              sylus        127    1.306988\n",
            "8            smiling        101    1.039415\n",
            "9              story        101    1.039415\n",
            "10              main         94    0.967377\n",
            "11            crying         92    0.946794\n",
            "12      heartgrowing         79    0.813008\n",
            "13         beautiful         78    0.802717\n",
            "14               omg         73    0.751261\n",
            "15             thank         68    0.699804\n",
            "16            please         68    0.699804\n",
            "17            barbie         62    0.638057\n",
            "18               yes         61    0.627766\n",
            "19           spotify         57    0.586601\n",
            "20              good         57    0.586601\n",
            "21            update         53    0.545436\n",
            "22              back         53    0.545436\n",
            "23  hearteyessmiling         51    0.524853\n",
            "24            infold         46    0.473397\n",
            "25             tears         45    0.463106\n",
            "26           holding         44    0.452815\n",
            "27    heartsparkling         43    0.442523\n",
            "28           finally         41    0.421941\n",
            "29              game         40    0.411650\n",
            "30             vibes         40    0.411650\n",
            "31            loudly         38    0.391067\n",
            "32              need         36    0.370485\n",
            "33         hearteyes         36    0.370485\n",
            "34               new         34    0.349902\n",
            "35              cant         34    0.349902\n",
            "36               two         33    0.339611\n",
            "37              blue         33    0.339611\n",
            "38             hands         33    0.339611\n",
            "39               god         32    0.329320\n",
            "40            taylor         32    0.329320\n",
            "41            hearts         32    0.329320\n",
            "42            giving         31    0.319029\n",
            "43              open         30    0.308737\n",
            "44           amazing         29    0.298446\n",
            "45             music         29    0.298446\n",
            "46            spring         29    0.298446\n",
            "47              feel         28    0.288155\n",
            "48        faceloudly         28    0.288155\n",
            "49           excited         27    0.277864\n",
            "\n",
            "Top 50 most frequent words in negative comments:\n",
            "               Word  Frequency  Percentage\n",
            "0            crying        410    4.638009\n",
            "1              face        285    3.223982\n",
            "2        faceloudly        205    2.319005\n",
            "3             zayne        144    1.628959\n",
            "4             sylus        143    1.617647\n",
            "5               omg        112    1.266968\n",
            "6            loudly        110    1.244344\n",
            "7             story        102    1.153846\n",
            "8              main         95    1.074661\n",
            "9            update         86    0.972851\n",
            "10          spotify         86    0.972851\n",
            "11             need         85    0.961538\n",
            "12              pls         70    0.791855\n",
            "13            heart         67    0.757919\n",
            "14             song         57    0.644796\n",
            "15      popperparty         53    0.599548\n",
            "16             cant         51    0.576923\n",
            "17              gun         50    0.565611\n",
            "18           infold         50    0.565611\n",
            "19           please         43    0.486425\n",
            "20             wait         40    0.452489\n",
            "21             open         40    0.452489\n",
            "22             skin         38    0.429864\n",
            "23             back         38    0.429864\n",
            "24             dont         37    0.418552\n",
            "25          holding         36    0.407240\n",
            "26         heartred         36    0.407240\n",
            "27            hands         35    0.395928\n",
            "28              two         35    0.395928\n",
            "29         question         32    0.361991\n",
            "30              new         31    0.350679\n",
            "31          thought         30    0.339367\n",
            "32            light         30    0.339367\n",
            "33        screaming         29    0.328054\n",
            "34           barbie         28    0.316742\n",
            "35              car         28    0.316742\n",
            "36             game         27    0.305430\n",
            "37        mouthface         26    0.294118\n",
            "38          finally         25    0.282805\n",
            "39             love         25    0.282805\n",
            "40            tears         24    0.271493\n",
            "41            going         24    0.271493\n",
            "42              god         24    0.271493\n",
            "43             holy         24    0.271493\n",
            "44            gonna         22    0.248869\n",
            "45              red         22    0.248869\n",
            "46              got         22    0.248869\n",
            "47       crossedout         21    0.237557\n",
            "48           popper         21    0.237557\n",
            "49  markexclamation         21    0.237557\n",
            "Word frequency visualization saved to: word_frequency.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-965627367.py:177: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.barplot(x=\"Frequency\", y=\"Word\", data=top_words, palette=\"viridis\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word frequency visualization saved to: word_frequency_positive.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-965627367.py:177: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.barplot(x=\"Frequency\", y=\"Word\", data=top_words, palette=\"viridis\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word frequency visualization saved to: word_frequency_negative.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Âü∫‰∫éLDAÊ®°ÂûãËøõË°åÂàÜÊûê"
      ],
      "metadata": {
        "id": "yhE6ymPbQ9f8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim pyLDAvis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qakg0RadQ_QU",
        "outputId": "786491a3-e37b-4b10-e819-196ac01b9ae3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (3.1.6)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (2.11.0)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (75.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->pyLDAvis) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora, models\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis"
      ],
      "metadata": {
        "id": "4kHs80Z1RBn5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_topics(sentiment_df, num_topics=5, passes=15):\n",
        "    \"\"\"\n",
        "    ‰ΩøÁî®LDAÊ®°ÂûãÂàÜÊûêËØÑËÆ∫‰∏ªÈ¢ò\n",
        "\n",
        "    ÂèÇÊï∞:\n",
        "        sentiment_df: ÊÉÖÊÑüÂàÜÊûêÁªìÊûúDataFrame\n",
        "        num_topics: ‰∏ªÈ¢òÊï∞Èáè\n",
        "        passes: ËÆ≠ÁªÉËø≠‰ª£Ê¨°Êï∞\n",
        "    \"\"\"\n",
        "    print(\"\\n=== ÂºÄÂßã‰∏ªÈ¢òÊ®°ÂûãÂàÜÊûê ===\")\n",
        "\n",
        "    # ÊñáÊú¨È¢ÑÂ§ÑÁêÜÔºàÈÄÇÁî®‰∫é‰∏ªÈ¢òÊ®°ÂûãÔºâ\n",
        "    all_texts = sentiment_df['processed_text'].tolist()\n",
        "\n",
        "    # ËøáÊª§Á©∫ÊñáÊú¨\n",
        "    valid_texts = [text for text in all_texts if text]\n",
        "\n",
        "    # ÂàõÂª∫ËØçÂÖ∏\n",
        "    dictionary = corpora.Dictionary(valid_texts)\n",
        "\n",
        "    # ËøáÊª§‰ΩéÈ¢ëËØçÂíåÈ´òÈ¢ëËØç\n",
        "    dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
        "\n",
        "    # ÂàõÂª∫ËØ≠ÊñôÂ∫ì\n",
        "    corpus = [dictionary.doc2bow(text) for text in valid_texts]\n",
        "\n",
        "    # ËÆ≠ÁªÉLDAÊ®°Âûã\n",
        "    lda_model = LdaModel(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary,\n",
        "        num_topics=num_topics,\n",
        "        passes=passes,\n",
        "        random_state=42,\n",
        "        alpha='auto',\n",
        "        eta='auto'\n",
        "    )\n",
        "\n",
        "    # ËØÑ‰º∞Ê®°Âûã\n",
        "    coherence_model = CoherenceModel(model=lda_model, texts=valid_texts,\n",
        "                                    dictionary=dictionary, coherence='c_v')\n",
        "    coherence = coherence_model.get_coherence()\n",
        "    perplexity = lda_model.log_perplexity(corpus)\n",
        "\n",
        "    print(f\"LDAÊ®°ÂûãËÆ≠ÁªÉÂÆåÊàêÔºå‰∏ªÈ¢òÊï∞Èáè: {num_topics}\")\n",
        "    print(f\"Ê®°ÂûãÂõ∞ÊÉëÂ∫¶: {perplexity:.4f}\")\n",
        "    print(f\"Ê®°Âûã‰∏ÄËá¥ÊÄßÂàÜÊï∞: {coherence:.4f}\")\n",
        "\n",
        "    # ‰øùÂ≠ò‰∏ªÈ¢òÂÖ≥ÈîÆËØç\n",
        "    topic_keywords = {}\n",
        "    for topic_id in range(num_topics):\n",
        "        keywords = lda_model.show_topic(topic_id, topn=10)\n",
        "        topic_keywords[topic_id + 1] = [word for word, _ in keywords]\n",
        "\n",
        "    # ÂèØËßÜÂåñ‰∏ªÈ¢ò\n",
        "    try:\n",
        "        vis_data = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "        pyLDAvis.save_html(vis_data, 'lda_visualization.html')\n",
        "        print(\"‰∏ªÈ¢òÂèØËßÜÂåñÁªìÊûúÂ∑≤‰øùÂ≠ò‰∏∫ 'lda_visualization.html'\")\n",
        "    except Exception as e:\n",
        "        print(f\"‰∏ªÈ¢òÂèØËßÜÂåñÂá∫Èîô: {str(e)}\")\n",
        "\n",
        "    return lda_model, topic_keywords, dictionary, corpus\n",
        "\n",
        "# Êñ∞Â¢ûÔºöÊó∂Èó¥Â∫èÂàóË∂ãÂäøÂàÜÊûê\n",
        "def analyze_time_series(sentiment_df, time_column='publish_time', window_days=30):\n",
        "    \"\"\"\n",
        "    ÂàÜÊûêÊÉÖÊÑüÈöèÊó∂Èó¥ÁöÑÂèòÂåñË∂ãÂäø\n",
        "\n",
        "    ÂèÇÊï∞:\n",
        "        sentiment_df: Â∏¶ÊÉÖÊÑüÂàÜÊûêÁªìÊûúÁöÑDataFrame\n",
        "        time_column: Êó∂Èó¥ÂàóÂêçÔºàÈúÄ‰∏∫datetimeÁ±ªÂûãÔºâ\n",
        "        window_days: Êó∂Èó¥Á™óÂè£Â§ßÂ∞èÔºàÂ§©Ôºâ\n",
        "    \"\"\"\n",
        "    print(\"\\n=== ÂºÄÂßãÊó∂Èó¥Â∫èÂàóÂàÜÊûê ===\")\n",
        "\n",
        "    # Á°Æ‰øùÊó∂Èó¥Âàó‰∏∫datetimeÁ±ªÂûã\n",
        "    if not pd.api.types.is_datetime64_any_dtype(sentiment_df[time_column]):\n",
        "        sentiment_df[time_column] = pd.to_datetime(sentiment_df[time_column])\n",
        "\n",
        "    # ÊåâÊó∂Èó¥ÊéíÂ∫è\n",
        "    sentiment_df = sentiment_df.sort_values(time_column)\n",
        "\n",
        "    # ÂàõÂª∫Êó∂Èó¥Á™óÂè£ÔºàÊØèÊúàÔºâ\n",
        "    sentiment_df['month'] = sentiment_df[time_column].dt.to_period('M')\n",
        "\n",
        "    # ËÆ°ÁÆóÊØèÊúàÊÉÖÊÑüÂàÜÂ∏É\n",
        "    monthly_sentiment = sentiment_df.groupby(['month', 'sentiment'])['confidence'].count().unstack(fill_value=0)\n",
        "\n",
        "    # ËÆ°ÁÆóÊØèÊúàÊÄªËØÑËÆ∫Êï∞\n",
        "    monthly_total = monthly_sentiment.sum(axis=1)\n",
        "\n",
        "    # ËÆ°ÁÆóÊÉÖÊÑüÊØî‰æã\n",
        "    monthly_proportion = monthly_sentiment.div(monthly_total, axis=0)\n",
        "\n",
        "    # ÂèØËßÜÂåñÊó∂Èó¥Ë∂ãÂäø\n",
        "    plt.figure(figsize=(14, 7))\n",
        "\n",
        "    # ÁªòÂà∂ÁßØÊûÅÊÉÖÊÑüË∂ãÂäø\n",
        "    if 'POSITIVE' in monthly_proportion.columns:\n",
        "        plt.plot(monthly_proportion.index.astype(str), monthly_proportion['POSITIVE'],\n",
        "                 marker='o', label='Positive', color='lightgreen')\n",
        "\n",
        "    # ÁªòÂà∂Ê∂àÊûÅÊÉÖÊÑüË∂ãÂäø\n",
        "    if 'NEGATIVE' in monthly_proportion.columns:\n",
        "        plt.plot(monthly_proportion.index.astype(str), monthly_proportion['NEGATIVE'],\n",
        "                 marker='o', label='Negative', color='salmon')\n",
        "\n",
        "    plt.title('Sentiment Trend Over Time')\n",
        "    plt.xlabel('Month')\n",
        "    plt.ylabel('Proportion')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('sentiment_time_series.png')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"ÊÉÖÊÑüÊó∂Èó¥Ë∂ãÂäøÂõæÂ∑≤‰øùÂ≠ò‰∏∫ 'sentiment_time_series.png'\")\n",
        "\n",
        "    return monthly_proportion\n",
        "\n",
        "# Êñ∞Â¢ûÔºöÊÉÖÊÑü-‰∏ªÈ¢òËÅîÂêàÂàÜÊûê\n",
        "def analyze_sentiment_topic(lda_model, sentiment_df, dictionary, corpus, top_n=5):\n",
        "    \"\"\"\n",
        "    ÂàÜÊûê‰∏çÂêåÊÉÖÊÑü‰∏ãÁöÑ‰∏ªÈ¢òÂàÜÂ∏É\n",
        "\n",
        "    ÂèÇÊï∞:\n",
        "        lda_model: LDAÊ®°Âûã\n",
        "        sentiment_df: ÊÉÖÊÑüÂàÜÊûêÁªìÊûú\n",
        "        dictionary: ËØçÂÖ∏\n",
        "        corpus: ËØ≠ÊñôÂ∫ì\n",
        "        top_n: ÊòæÁ§∫ÁöÑ‰∏ªÈ¢òÊï∞Èáè\n",
        "    \"\"\"\n",
        "    print(\"\\n=== ÂºÄÂßãÊÉÖÊÑü-‰∏ªÈ¢òËÅîÂêàÂàÜÊûê ===\")\n",
        "\n",
        "    # ÊåâÊÉÖÊÑüÂàÜÁ±ª\n",
        "    positive_df = sentiment_df[sentiment_df['sentiment'] == 'POSITIVE']\n",
        "    negative_df = sentiment_df[sentiment_df['sentiment'] == 'NEGATIVE']\n",
        "\n",
        "    # ÊèêÂèñÁßØÊûÅËØÑËÆ∫ÁöÑËØ≠ÊñôÂ∫ì\n",
        "    positive_indices = sentiment_df[sentiment_df['sentiment'] == 'POSITIVE'].index\n",
        "    positive_corpus = [corpus[i] for i in positive_indices if i < len(corpus)]\n",
        "\n",
        "    # ÊèêÂèñÊ∂àÊûÅËØÑËÆ∫ÁöÑËØ≠ÊñôÂ∫ì\n",
        "    negative_indices = sentiment_df[sentiment_df['sentiment'] == 'NEGATIVE'].index\n",
        "    negative_corpus = [corpus[i] for i in negative_indices if i < len(corpus)]\n",
        "\n",
        "    # ËÆ°ÁÆóÁßØÊûÅËØÑËÆ∫ÁöÑ‰∏ªÈ¢òÂàÜÂ∏É\n",
        "    positive_topic_dist = np.zeros(lda_model.num_topics)\n",
        "    for doc in positive_corpus:\n",
        "        topics = lda_model.get_document_topics(doc)\n",
        "        for topic, prob in topics:\n",
        "            positive_topic_dist[topic] += prob\n",
        "\n",
        "    # ËÆ°ÁÆóÊ∂àÊûÅËØÑËÆ∫ÁöÑ‰∏ªÈ¢òÂàÜÂ∏É\n",
        "    negative_topic_dist = np.zeros(lda_model.num_topics)\n",
        "    for doc in negative_corpus:\n",
        "        topics = lda_model.get_document_topics(doc)\n",
        "        for topic, prob in topics:\n",
        "            negative_topic_dist[topic] += prob\n",
        "\n",
        "    # ÂΩí‰∏ÄÂåñ\n",
        "    if len(positive_corpus) > 0:\n",
        "        positive_topic_dist /= len(positive_corpus)\n",
        "\n",
        "    if len(negative_corpus) > 0:\n",
        "        negative_topic_dist /= len(negative_corpus)\n",
        "\n",
        "    # ÂàõÂª∫‰∏ªÈ¢òÂàÜÂ∏ÉDataFrame\n",
        "    topic_df = pd.DataFrame({\n",
        "        'Topic': range(1, lda_model.num_topics + 1),\n",
        "        'Positive_Proportion': positive_topic_dist,\n",
        "        'Negative_Proportion': negative_topic_dist\n",
        "    })\n",
        "\n",
        "    # ÂèØËßÜÂåñÊÉÖÊÑü-‰∏ªÈ¢òÂàÜÂ∏É\n",
        "    plt.figure(figsize=(12, 7))\n",
        "\n",
        "    x = topic_df['Topic']\n",
        "    width = 0.35\n",
        "\n",
        "    plt.bar(x - width/2, topic_df['Positive_Proportion'], width, label='Positive', color='lightgreen')\n",
        "    plt.bar(x + width/2, topic_df['Negative_Proportion'], width, label='Negative', color='salmon')\n",
        "\n",
        "    plt.title('Topic Distribution by Sentiment')\n",
        "    plt.xlabel('Topic')\n",
        "    plt.ylabel('Proportion')\n",
        "    plt.xticks(x)\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('sentiment_topic_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"ÊÉÖÊÑü-‰∏ªÈ¢òÂàÜÂ∏ÉÂõæÂ∑≤‰øùÂ≠ò‰∏∫ 'sentiment_topic_distribution.png'\")\n",
        "\n",
        "    # ËæìÂá∫ÂêÑÊÉÖÊÑü‰∏ãÁöÑÁÉ≠Èó®‰∏ªÈ¢ò\n",
        "    print(\"\\nÁßØÊûÅËØÑËÆ∫‰∏≠ÁöÑÁÉ≠Èó®‰∏ªÈ¢ò:\")\n",
        "    for i, topic in enumerate(topic_df.sort_values('Positive_Proportion', ascending=False).head(top_n).itertuples()):\n",
        "        print(f\"‰∏ªÈ¢ò {topic.Topic}: ÊØî‰æã {topic.Positive_Proportion:.2%}\")\n",
        "\n",
        "    print(\"\\nÊ∂àÊûÅËØÑËÆ∫‰∏≠ÁöÑÁÉ≠Èó®‰∏ªÈ¢ò:\")\n",
        "    for i, topic in enumerate(topic_df.sort_values('Negative_Proportion', ascending=False).head(top_n).itertuples()):\n",
        "        print(f\"‰∏ªÈ¢ò {topic.Topic}: ÊØî‰æã {topic.Negative_Proportion:.2%}\")\n",
        "\n",
        "    return topic_df\n",
        "\n",
        "# Âú®Âéü‰ª£Á†ÅÊú´Â∞æÊ∑ªÂä†Êñ∞ÂäüËÉΩË∞ÉÁî®\n",
        "try:\n",
        "    print(\"\\nÂºÄÂßãÈ´òÁ∫ßÂàÜÊûê...\")\n",
        "\n",
        "    # 1. LDA‰∏ªÈ¢òÊ®°ÂûãÂàÜÊûê\n",
        "    lda_model, topic_keywords, dictionary, corpus = analyze_topics(results_df, num_topics=5)\n",
        "\n",
        "    # ËæìÂá∫‰∏ªÈ¢òÂÖ≥ÈîÆËØç\n",
        "    print(\"\\n=== ‰∏ªÈ¢òÂÖ≥ÈîÆËØç ===\")\n",
        "    for topic_id, keywords in topic_keywords.items():\n",
        "        print(f\"‰∏ªÈ¢ò {topic_id}: {' '.join(keywords[:10])}\")\n",
        "\n",
        "    # 2. Êó∂Èó¥Â∫èÂàóË∂ãÂäøÂàÜÊûêÔºàÂÅáËÆæÊï∞ÊçÆÂåÖÂê´publish_timeÂàóÔºåÈúÄÊ†πÊçÆÂÆûÈôÖÊï∞ÊçÆË∞ÉÊï¥Ôºâ\n",
        "    # Ê≥®ÊÑèÔºöËã•Êï∞ÊçÆ‰∏≠Ê≤°ÊúâÊó∂Èó¥ÂàóÔºåÈúÄÂÖàÊ∑ªÂä†ÊàñË∑≥ËøáÊ≠§Ê≠•È™§\n",
        "    if 'publish_time' in merged_df.columns:\n",
        "        monthly_proportion = analyze_time_series(results_df, time_column='publish_time')\n",
        "    else:\n",
        "        print(\"Ë≠¶Âëä: Êï∞ÊçÆ‰∏≠Áº∫Â∞ëÊó∂Èó¥ÂàóÔºåË∑≥ËøáÊó∂Èó¥Â∫èÂàóÂàÜÊûê\")\n",
        "\n",
        "    # 3. ÊÉÖÊÑü-‰∏ªÈ¢òËÅîÂêàÂàÜÊûê\n",
        "    if 'lda_model' in locals():  # Á°Æ‰øùLDAÊ®°ÂûãÂ∑≤ÂàõÂª∫\n",
        "        topic_df = analyze_sentiment_topic(lda_model, results_df, dictionary, corpus)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"È´òÁ∫ßÂàÜÊûêÂá∫Èîô: {str(e)}\")\n",
        "\n",
        "print(\"\\nÂÆåÊï¥ÂàÜÊûêÂÆåÊàê!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E89pAWzoRuIn",
        "outputId": "6c3b316d-761b-43f8-fb94-b20933ae106f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ÂºÄÂßãÈ´òÁ∫ßÂàÜÊûê...\n",
            "\n",
            "=== ÂºÄÂßã‰∏ªÈ¢òÊ®°ÂûãÂàÜÊûê ===\n",
            "È´òÁ∫ßÂàÜÊûêÂá∫Èîô: doc2bow expects an array of unicode tokens on input, not a single string\n",
            "\n",
            "ÂÆåÊï¥ÂàÜÊûêÂÆåÊàê!\n"
          ]
        }
      ]
    }
  ]
}